{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network in Numpy\n",
    "\n",
    "Building a simple neural network from scratch with numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic functionalities to implement\n",
    "\n",
    "- Neuron class and $\\sum_i^{n}f(w_i \\cdot x_i + b)$ calculation\n",
    "- Feed forward method\n",
    "- Loss calculation method\n",
    "- Back propagation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statistics\n",
    "from sklearn.datasets import make_classification, make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return np.exp(x) / (np.exp(x)+1)\n",
    "\n",
    "def softmax(x_i, x):\n",
    "    return np.exp(x_i)/np.sum(np.exp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions\n",
    "\n",
    "def cross_entropy(x):\n",
    "    return -np.log(x)\n",
    "\n",
    "def cross_entropy_mult(x_i, x):\n",
    "    return -np.log(softmax(x_i, x))\n",
    "\n",
    "def mse(y_pred, y_actuals):\n",
    "    s = (np.square(y_pred-y_actuals))\n",
    "    s = np.sum(s)/len(y_actuals)\n",
    "    return(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dummy classification dataset\n",
    "X, y = make_classification(n_samples=500, n_features = 10, n_informative = 3, n_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    \"\"\"\n",
    "    Class for a simple multi-layer perceptron neural network.\n",
    "    Implements feed forward, loss and back propagation methods.\n",
    "    \n",
    "    loss_function method must have two arguments for both predicted and actual values.\n",
    "    \n",
    "    Some code referenced from:\n",
    "        https://www.geeksforgeeks.org/implementation-of-neural-network-from-scratch-using-numpy/\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes, loss_function, activation, tol = 0.001, epochs = 100):\n",
    "        \n",
    "        # Initialisation of a simple neural network.\n",
    "        if type(layer_sizes) not in [tuple, list, np.ndarray]:\n",
    "            raise ValueError(\"layer_sizes must be one of tuple, list, np.ndarray\")\n",
    "            \n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.loss_function = loss_function\n",
    "        self.tol = tol\n",
    "        self.epochs = 100\n",
    "        self.weights = None\n",
    "        self.input_layer_size = None\n",
    "        self.num_hidden_layers = self.__get_number_of_hidden_layers()\n",
    "        self.losses = list()\n",
    "        self.activation = activation\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the neural network using some data X and y.\n",
    "        \"\"\"\n",
    "        self.input_layer_size = X.shape[1]\n",
    "        self.__initialise_weights()\n",
    "\n",
    "        # iterate through epochs to train\n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            epoch_loss = []\n",
    "            for x_i, y_i in zip(X, y):\n",
    "                \n",
    "                output = self.__feed_forward(x_i, sigmoid)\n",
    "                \n",
    "                # calculate the instance loss\n",
    "                loss = self.__loss(y_i, output, f = self.loss_function)\n",
    "                epoch_loss.append(loss)\n",
    "                \n",
    "                #self.weights = self.__back_propagation(y_i, output, learning_rate = 0.1)\n",
    "                \n",
    "            # append the average loss across the epoch\n",
    "            self.losses.append(statistics.mean(epoch_loss))\n",
    "            \n",
    "        #print(self.weights)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Create predictions using the trained neural network.\n",
    "        \"\"\"\n",
    "        \n",
    "        # check if network is fitted or not. Raise error if not.\n",
    "        self.__is_network_fitted()\n",
    "        \n",
    "        outputs = np.array([])\n",
    "        for x_i in X:\n",
    "            outputs = np.append(outputs, self.__feed_forward(x_i, self.activation))\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "        \n",
    "    def score(X, y):\n",
    "        \"\"\"\n",
    "        Use some score metric to score the accuracy of the neural network.\n",
    "        \"\"\"\n",
    "        \n",
    "        # check if network is fitted or not. Raise error if not.\n",
    "        self.__is_network_fitted()\n",
    "\n",
    "\n",
    "    def __feed_forward(self, x_i, activation):\n",
    "        \"\"\"\n",
    "        Feed input forward through the neural network.\n",
    "        \"\"\"\n",
    "        output = x_i\n",
    "        for layer_idx in range(self.num_hidden_layers):\n",
    "            output = activation(self.weights['w'+str(layer_idx)].T.dot(output) + self.weights['b'+str(layer_idx)])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def __initialise_weights(self, seed = 42):\n",
    "        \"\"\"\n",
    "        Randomly initialise the weights and biases for all layers in the network.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        input_layer_size = self.input_layer_size\n",
    "        \n",
    "        param_values = {}\n",
    "        \n",
    "        # set input layer sizes\n",
    "        param_values['w0'] = np.random.rand(self.input_layer_size, self.layer_sizes[0]) * 0.1\n",
    "        param_values['b0'] = np.random.rand(self.layer_sizes[0]) * 0.1\n",
    "\n",
    "        for layer_i_minus1, layer_i in zip(np.array(range(0, self.num_hidden_layers-1)), np.array(range(1,self.num_hidden_layers))):\n",
    "            param_values['w' + str(layer_i)] = np.random.randn(self.layer_sizes[layer_i_minus1], self.layer_sizes[layer_i]) * 0.1\n",
    "            param_values['b' + str(layer_i)] = np.random.randn(self.layer_sizes[layer_i]) * 0.1\n",
    "\n",
    "        self.weights = param_values\n",
    "        \n",
    "    def __loss(self, y_actuals, y_pred, f):\n",
    "        return f(y_actuals, y_pred)\n",
    "    \n",
    "    def __get_number_of_hidden_layers(self):\n",
    "        \"\"\"\n",
    "        Get the number of hidden layers in the network from self.layer_sizes\n",
    "        \"\"\"\n",
    "        # initiliase all neurons\n",
    "        if type(self.layer_sizes) in [list, tuple]:\n",
    "            return len(self.layer_sizes)\n",
    "            \n",
    "        elif type(self.layer_sizes) is np.ndarray:\n",
    "            return self.layer_sizes.size\n",
    "    \n",
    "        else: \n",
    "            return 1\n",
    "    \n",
    "    def __back_propagation(self, y_i, output, learning_rate = 0.1):\n",
    "        \"\"\"\n",
    "        Back propagation algorithm for tuning weights in the neural network.\n",
    "        \"\"\"\n",
    "        return None\n",
    "    \n",
    "    def __is_network_fitted(self):\n",
    "        \"\"\"\n",
    "        Check if network is fitted or not.\n",
    "        \"\"\"\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"Network has not been fitted. Fit the network using `.fit()`\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mlp = MLP([2,3,2], loss_function=mse, activation = relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mlp.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mlp.predict(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron():\n",
    "    \"\"\"\n",
    "    Class to construct a neuron. \n",
    "    Neuron is defined by weights, data `x` and biases `b` passed through an activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, w, x, b, activation):\n",
    "        \n",
    "        # initialisation of a neuron\n",
    "        self.w = w\n",
    "        self.x = x\n",
    "        self.b = b\n",
    "        self.activation = activation\n",
    "        \n",
    "    def calc(self):\n",
    "        \n",
    "        # calculate output for the neuron\n",
    "        neuron = self.x.dot(self.w) + self.b\n",
    "        return self.activation(neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.8"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing neuron\n",
    "test_w = np.array([3,1,-2])\n",
    "test_x = np.array([0.2, 9, 1])\n",
    "test_b = 0.2\n",
    "\n",
    "x = Neuron(test_w, test_x, test_b, relu)\n",
    "x.calc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
